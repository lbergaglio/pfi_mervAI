import json
import time
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.edge.service import Service
from selenium.webdriver.edge.options import Options
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException, TimeoutException
import pickle
import logging

URL = "https://www.ambito.com/finanzas/"
COOKIES_PATH = "cookies.pkl"
TIME_SLEEP = 5


# Configuraci√≥n b√°sica de logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# === Clasificaci√≥n autom√°tica por palabras clave ===
CATEGORIAS = {
    "d√≥lar": ["d√≥lar", "d√≥lares", "blue", "oficial", "mayorista", "MEP", "CCL"],
    "acciones": ["acci√≥n", "acciones", "ADR", "renta variable"],
    "bonos": ["bonos", "bono", "t√≠tulo", "deuda", "AL30", "GD30", "cupones"],
    "Merval": ["Merval", "√≠ndice", "bolsa porte√±a"],
    "macro": ["inflaci√≥n", "actividad", "PIB", "econom√≠a", "recesi√≥n", "macroeconom√≠a", "macroecon√≥mico", "super√°vit"],
    "internacional": ["Wall Street", "EEUU", "China", "Brasil", "global", "internacional", "Reserva Federal", "Fed"]
}

def clasificar_texto(texto):
    texto = texto.lower()
    categorias_detectadas = []
    for categoria, palabras in CATEGORIAS.items():
        if any(palabra.lower() in texto for palabra in palabras):
            categorias_detectadas.append(categoria)
    return categorias_detectadas if categorias_detectadas else ["otras"]

def cargar_cookies(driver, path=COOKIES_PATH):
    try:
        with open(path, "rb") as file:
            cookies = pickle.load(file)
            for cookie in cookies:
                driver.add_cookie(cookie)
        logging.info(f"üç™ {len(cookies)} cookies cargadas correctamente desde {path}.")
    except FileNotFoundError:
        logging.warning(f"‚ö†Ô∏è El archivo de cookies no se encontr√≥ en '{path}'. Se continuar√° sin cookies.")
    except Exception as e:
        logging.error(f"‚ö†Ô∏è No se pudieron cargar las cookies desde '{path}': {e}")

def scrapear_ambito():
    edge_options = Options()
    edge_options.add_argument("--start-maximized")
    # Es mejor no hardcodear la ruta al driver, Selenium puede manejarlo si est√° en el PATH
    service = Service() 
    driver = webdriver.Edge(service=service, options=edge_options)

    driver.get(URL)
    time.sleep(TIME_SLEEP)

    cargar_cookies(driver)
    driver.refresh()
    time.sleep(TIME_SLEEP)

    articles = driver.find_elements(By.CSS_SELECTOR, "article a[href*='/finanzas/']")
    links = []

    for article in articles:
        href = article.get_attribute("href")
        if href and href not in links:
            links.append(href)

    links = links[:8]
    noticias = []

    for url in links:
        logging.info(f"üîó Entrando a: {url}")
        try:
            driver.get(url)
            time.sleep(TIME_SLEEP)
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(TIME_SLEEP)

            titulo = driver.find_element(By.TAG_NAME, "h1").text

            contenido = ""
            posibles_selectores = [
                ".article-main-content",
                ".article-body",
                "div.article-content",
                "div.article-text"
            ]
            for selector in posibles_selectores:
                elementos = driver.find_elements(By.CSS_SELECTOR, f"{selector} p")
                if elementos:
                    contenido = "\n".join([el.text for el in elementos if el.text.strip()])
                    break

            if not contenido:
                contenido = "[Contenido no encontrado]"

            tags = [tag.text for tag in driver.find_elements(By.CSS_SELECTOR, ".tags a")]
            fecha = datetime.now().isoformat()

            if "Registrate gratis" in contenido or "superaste el l√≠mite" in contenido.lower():
                contenido = "[Bloqueado por muro de pago]"

            categorias = clasificar_texto(titulo + " " + contenido)

            noticia = {
                "fecha": fecha,
                "tags": tags,
                "titulo": titulo,
                "url": url,
                "contenido": contenido.strip(),
                "categorias": categorias
            }

            noticias.append(noticia)
            logging.info(f"üóûÔ∏è Noticia obtenida: {titulo}")
            # print(f"üóûÔ∏è {fecha} | {tags}")
            # print(f"üìå {titulo}")
            # print(f"üîó {url}")
            # print(f"üè∑Ô∏è Categor√≠as: {categorias}")
            # print(f"üìù {contenido[:250]}...\n" + "-" * 120)

        except (NoSuchElementException, TimeoutException) as e:
            logging.warning(f"No se pudo encontrar un elemento en {url} o el tiempo de espera se agot√≥. Error: {type(e).__name__}")
        except Exception as e:
            logging.error(f"Error inesperado al procesar la nota {url}: {e}", exc_info=True)

    with open("noticias_ambito.json", "w", encoding="utf-8") as f:
        json.dump(noticias, f, indent=2, ensure_ascii=False)
        logging.info(f"\nüíæ {len(noticias)} noticias guardadas en noticias_ambito.json")

    driver.quit()

if __name__ == "__main__":
    scrapear_ambito()
